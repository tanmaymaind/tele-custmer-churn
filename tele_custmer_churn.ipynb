{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyNAGAxR6+f8WcoBoTrYydI/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanmaymaind/desktop-tutorial/blob/main/tele_custmer_churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYFFE4xOWjUF"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pandas library, which is essential for working with dataframes\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to your CSV file\n",
        "# The dataset was downloaded to /kaggle/input/telco-customer-churn\n",
        "file_path = '/content/WA_Fn-UseC_-Telco-Customer-Churn[1].csv'\n",
        "\n",
        "# Load the dataset into a pandas DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first 5 rows of the DataFrame to get a quick look at the data\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Get a concise summary of the DataFrame, including data types and missing values\n",
        "print(\"\\nDataset information:\")\n",
        "df.info()\n",
        "\n",
        "# Get descriptive statistics for numerical columns\n",
        "print(\"\\nDescriptive statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Check for the number of missing values in each column\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "XYBXJpnaWo-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'TotalCharges' to a numeric type.\n",
        "# errors='coerce' will replace any values that cannot be converted to numeric with NaN (Not a Number)\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "# Now, let's check for missing values again, specifically in 'TotalCharges'\n",
        "print(\"Missing values in 'TotalCharges' after conversion:\")\n",
        "print(df['TotalCharges'].isnull().sum())\n",
        "\n",
        "# Let's see the data type of TotalCharges now\n",
        "print(\"\\nData type of 'TotalCharges' after conversion:\")\n",
        "print(df['TotalCharges'].dtype)"
      ],
      "metadata": {
        "id": "_rTCFmE8WpBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display rows where 'TotalCharges' is NaN to understand the context\n",
        "print(\"\\nRows where TotalCharges is NaN:\")\n",
        "print(df[df['TotalCharges'].isnull()][['customerID', 'tenure', 'MonthlyCharges', 'TotalCharges']])\n",
        "\n",
        "# Count how many missing values we have\n",
        "missing_total_charges_count = df['TotalCharges'].isnull().sum()\n",
        "\n",
        "if missing_total_charges_count > 0:\n",
        "    # For this specific dataset, it's common that missing TotalCharges are for customers with 0 tenure.\n",
        "    # In such cases, TotalCharges should logically be 0 or close to MonthlyCharges if tenure is 1.\n",
        "    # For simplicity, if tenure is 0, TotalCharges should be 0.\n",
        "    # Let's check if these NaNs correspond to tenure = 0\n",
        "    if not df[(df['TotalCharges'].isnull()) & (df['tenure'] == 0)].empty:\n",
        "        print(\"\\nMissing TotalCharges are for customers with 0 tenure. Imputing with 0.\")\n",
        "        df['TotalCharges'] = df['TotalCharges'].fillna(0)\n",
        "    else:\n",
        "        # If NaNs are not just for tenure 0, we might use median imputation\n",
        "        print(\"\\nMissing TotalCharges found for tenures other than 0, or tenure 0 check is inconclusive.\")\n",
        "        print(\"Imputing with the median TotalCharge.\")\n",
        "        median_total_charges = df['TotalCharges'].median()\n",
        "        df['TotalCharges'] = df['TotalCharges'].fillna(median_total_charges)\n",
        "\n",
        "# Verify that there are no more missing values in 'TotalCharges'\n",
        "print(\"\\nMissing values in 'TotalCharges' after handling:\")\n",
        "print(df['TotalCharges'].isnull().sum())"
      ],
      "metadata": {
        "id": "Rm7OU3O9WpD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the unique values in the 'Churn' column\n",
        "print(\"\\nUnique values in 'Churn' column before conversion:\")\n",
        "print(df['Churn'].unique())\n",
        "\n",
        "# Convert 'Churn' column to numerical (0 or 1)\n",
        "# We can use map or replace, or scikit-learn's LabelEncoder later for many columns\n",
        "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Verify the conversion\n",
        "print(\"\\nUnique values in 'Churn' column after conversion:\")\n",
        "print(df['Churn'].unique())\n",
        "print(\"\\nData type of 'Churn' column after conversion:\")\n",
        "print(df['Churn'].dtype)\n",
        "\n",
        "# Let's look at the info again to see the updated Dtypes\n",
        "print(\"\\nDataset information after conversions:\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "0-LE8qH2WpIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set a style for seaborn plots for better aesthetics\n",
        "sns.set(style=\"whitegrid\")"
      ],
      "metadata": {
        "id": "eHVYOkOBWpKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the distribution of the 'Churn' variable\n",
        "churn_distribution = df['Churn'].value_counts(normalize=True) * 100\n",
        "print(\"Churn Distribution (%):\\n\", churn_distribution)\n",
        "\n",
        "# Visualize the Churn distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='Churn', data=df)\n",
        "plt.title('Distribution of Customer Churn (0 = No, 1 = Yes)')\n",
        "plt.xlabel('Churn')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.xticks([0, 1], ['No (0)', 'Yes (1)']) # To make x-axis labels clearer\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "27n8f_3XWpNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "for feature in numerical_features:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot distribution for all customers\n",
        "    plt.subplot(1, 2, 1) # 1 row, 2 columns, 1st subplot\n",
        "    sns.histplot(df[feature], kde=True, color='skyblue')\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    # Plot distribution separately for Churn vs. No Churn\n",
        "    plt.subplot(1, 2, 2) # 1 row, 2 columns, 2nd subplot\n",
        "    sns.histplot(df[df['Churn'] == 0][feature], label='Churn: No', kde=True, color='green', stat=\"density\", common_norm=False)\n",
        "    sns.histplot(df[df['Churn'] == 1][feature], label='Churn: Yes', kde=True, color='red', stat=\"density\", common_norm=False)\n",
        "    plt.title(f'Distribution of {feature} by Churn')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout() # Adjusts plot to prevent labels from overlapping\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "UdqkrT5UYvm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify categorical columns (excluding customerID and already numeric/target columns)\n",
        "categorical_features = df.select_dtypes(include='object').columns.tolist()\n",
        "# We can also manually list them if preferred, or if some 'object' columns are not truly categorical\n",
        "# For now, let's exclude 'customerID' from this general categorical analysis\n",
        "if 'customerID' in categorical_features:\n",
        "    categorical_features.remove('customerID')\n",
        "\n",
        "print(f\"\\nCategorical features to analyze: {categorical_features}\")\n",
        "\n",
        "for feature in categorical_features:\n",
        "    plt.figure(figsize=(10, 5)) # Adjust figsize as needed, some features have many categories\n",
        "    sns.countplot(x=feature, hue='Churn', data=df, palette=['green', 'red'])\n",
        "    plt.title(f'Churn Counts by {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Number of Customers')\n",
        "    plt.xticks(rotation=45, ha='right') # Rotate x-axis labels if they overlap\n",
        "    plt.legend(title='Churn', labels=['No', 'Yes'])\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "eBvKdvSVYvjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the customerID column as it's not needed for modeling\n",
        "df_processed = df.copy() # Create a copy to keep the original df intact for reference\n",
        "df_processed = df_processed.drop('customerID', axis=1)\n",
        "\n",
        "print(\"Columns after dropping customerID:\")\n",
        "print(df_processed.columns)\n",
        "print(f\"\\nShape of DataFrame after dropping customerID: {df_processed.shape}\")"
      ],
      "metadata": {
        "id": "C0cZsMKyYvg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify remaining categorical columns (object Dtype) to be encoded\n",
        "categorical_cols_to_encode = df_processed.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "print(f\"\\nCategorical columns to be one-hot encoded: {categorical_cols_to_encode}\")\n",
        "\n",
        "# Apply one-hot encoding using pandas get_dummies\n",
        "df_processed = pd.get_dummies(df_processed, columns=categorical_cols_to_encode, drop_first=True)\n",
        "\n",
        "print(\"\\nShape of DataFrame after one-hot encoding:\")\n",
        "print(df_processed.shape)\n",
        "\n",
        "print(\"\\nFirst 5 rows of the processed DataFrame (showing new encoded columns):\")\n",
        "print(df_processed.head())\n",
        "\n",
        "print(\"\\nDataset information of the processed DataFrame:\")\n",
        "df_processed.info()"
      ],
      "metadata": {
        "id": "mubllOyHYvcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (X) and target (y)\n",
        "# X will contain all columns except 'Churn'\n",
        "X = df_processed.drop('Churn', axis=1)\n",
        "\n",
        "# y will contain only the 'Churn' column\n",
        "y = df_processed['Churn']\n",
        "\n",
        "print(\"Shape of features (X):\", X.shape)\n",
        "print(\"Shape of target (y):\", y.shape)"
      ],
      "metadata": {
        "id": "e7YBSok9YvV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and testing sets (e.g., 80% train, 20% test)\n",
        "# random_state ensures reproducibility of the split\n",
        "# stratify=y ensures that the class proportions are maintained in train and test splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"\\nShape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "\n",
        "print(\"\\nChurn distribution in original data:\\n\", y.value_counts(normalize=True))\n",
        "print(\"Churn distribution in y_train:\\n\", y_train.value_counts(normalize=True))\n",
        "print(\"Churn distribution in y_test:\\n\", y_test.value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "FeZitgRDaQs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Identify numerical columns that need scaling\n",
        "# 'SeniorCitizen' is already 0/1, and one-hot encoded columns are 0/1.\n",
        "cols_to_scale = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data for these columns and transform them\n",
        "X_train[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])\n",
        "\n",
        "# Use the SAME fitted scaler to transform the test data for these columns\n",
        "X_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
        "\n",
        "print(\"\\nFirst 5 rows of X_train after scaling (showing only scaled columns for brevity):\")\n",
        "print(X_train[cols_to_scale].head())\n",
        "\n",
        "print(\"\\nFirst 5 rows of X_test after scaling (showing only scaled columns for brevity):\")\n",
        "print(X_test[cols_to_scale].head())"
      ],
      "metadata": {
        "id": "qPzB9LuEaQwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary models and metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, classification_report\n",
        "\n",
        "# Create a dictionary to store the results of each model\n",
        "model_results = {}\n",
        "\n",
        "# --- 1. Logistic Regression ---\n",
        "print(\"--- Logistic Regression ---\")\n",
        "log_reg = LogisticRegression(solver='liblinear', random_state=42) # liblinear is good for small datasets\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "y_pred_proba_log_reg = log_reg.predict_proba(X_test)[:, 1] # Probabilities for ROC AUC\n",
        "\n",
        "# Evaluate\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_log_reg)\n",
        "precision_lr = precision_score(y_test, y_pred_log_reg)\n",
        "recall_lr = recall_score(y_test, y_pred_log_reg)\n",
        "f1_lr = f1_score(y_test, y_pred_log_reg)\n",
        "roc_auc_lr = roc_auc_score(y_test, y_pred_proba_log_reg)\n",
        "model_results['Logistic Regression'] = {'Accuracy': accuracy_lr, 'Precision': precision_lr, 'Recall': recall_lr, 'F1-Score': f1_lr, 'ROC AUC': roc_auc_lr}\n",
        "\n",
        "print(f\"Accuracy: {accuracy_lr:.4f}\")\n",
        "print(f\"Precision: {precision_lr:.4f}\")\n",
        "print(f\"Recall: {recall_lr:.4f}\") # How many of the actual positives our model captured\n",
        "print(f\"F1-Score: {f1_lr:.4f}\")\n",
        "print(f\"ROC AUC Score: {roc_auc_lr:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log_reg))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_log_reg))\n",
        "\n",
        "\n",
        "# --- 2. Decision Tree Classifier ---\n",
        "print(\"\\n--- Decision Tree Classifier ---\")\n",
        "dt_clf = DecisionTreeClassifier(random_state=42) # Using default parameters for now\n",
        "dt_clf.fit(X_train, y_train)\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "y_pred_proba_dt = dt_clf.predict_proba"
      ],
      "metadata": {
        "id": "-otyXV_daQyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure these are imported\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, classification_report\n",
        "\n",
        "# Initialize (or re-initialize) the results dictionary\n",
        "model_results = {}"
      ],
      "metadata": {
        "id": "EPI6lKjiaQ0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Logistic Regression ---\n",
        "print(\"--- Logistic Regression ---\")\n",
        "log_reg = LogisticRegression(solver='liblinear', random_state=42)\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "y_pred_proba_log_reg = log_reg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_log_reg)\n",
        "precision_lr = precision_score(y_test, y_pred_log_reg)\n",
        "recall_lr = recall_score(y_test, y_pred_log_reg)\n",
        "f1_lr = f1_score(y_test, y_pred_log_reg)\n",
        "roc_auc_lr = roc_auc_score(y_test, y_pred_proba_log_reg)\n",
        "model_results['Logistic Regression'] = {'Accuracy': accuracy_lr, 'Precision': precision_lr, 'Recall': recall_lr, 'F1-Score': f1_lr, 'ROC AUC': roc_auc_lr}\n",
        "\n",
        "print(f\"Accuracy: {accuracy_lr:.4f}\")\n",
        "print(f\"Precision: {precision_lr:.4f}\")\n",
        "print(f\"Recall: {recall_lr:.4f}\")\n",
        "print(f\"F1-Score: {f1_lr:.4f}\")\n",
        "print(f\"ROC AUC Score: {roc_auc_lr:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log_reg))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_log_reg))"
      ],
      "metadata": {
        "id": "vQFvTjhiaQ4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Decision Tree Classifier ---\n",
        "print(\"\\n--- Decision Tree Classifier ---\")\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "y_pred_proba_dt = dt_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "precision_dt = precision_score(y_test, y_pred_dt)\n",
        "recall_dt = recall_score(y_test, y_pred_dt)\n",
        "f1_dt = f1_score(y_test, y_pred_dt)\n",
        "roc_auc_dt = roc_auc_score(y_test, y_pred_proba_dt)\n",
        "model_results['Decision Tree'] = {'Accuracy': accuracy_dt, 'Precision': precision_dt, 'Recall': recall_dt, 'F1-Score': f1_dt, 'ROC AUC': roc_auc_dt}\n",
        "\n",
        "print(f\"Accuracy: {accuracy_dt:.4f}\")\n",
        "print(f\"Precision: {precision_dt:.4f}\")\n",
        "print(f\"Recall: {recall_dt:.4f}\")\n",
        "print(f\"F1-Score: {f1_dt:.4f}\")\n",
        "print(f\"ROC AUC Score: {roc_auc_dt:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_dt))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_dt))"
      ],
      "metadata": {
        "id": "6NRL97REYvTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Random Forest Classifier ---\n",
        "print(\"\\n--- Random Forest Classifier ---\")\n",
        "# Ensure X_train, y_train, X_test, y_test are available from your previous data splitting step\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1) # n_estimators is the number of trees, n_jobs=-1 uses all processors\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "y_pred_proba_rf = rf_clf.predict_proba(X_test)[:, 1] # Probabilities for ROC AUC\n",
        "\n",
        "# Evaluate\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "precision_rf = precision_score(y_test, y_pred_rf)\n",
        "recall_rf = recall_score(y_test, y_pred_rf)\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "roc_auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
        "\n",
        "# Storing results - make sure model_results dictionary was initialized earlier\n",
        "if 'model_results' not in globals():\n",
        "    model_results = {} # Initialize if it wasn't\n",
        "    # If you ran LR and DT in this session, you'd ideally repopulate model_results\n",
        "    # For now, let's assume it exists or will be primarily built from these individual runs\n",
        "\n",
        "model_results['Random Forest'] = {'Accuracy': accuracy_rf, 'Precision': precision_rf, 'Recall': recall_rf, 'F1-Score': f1_rf, 'ROC AUC': roc_auc_rf}\n",
        "\n",
        "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
        "print(f\"Precision: {precision_rf:.4f}\")\n",
        "print(f\"Recall: {recall_rf:.4f}\")\n",
        "print(f\"F1-Score: {f1_rf:.4f}\")\n",
        "print(f\"ROC AUC Score: {roc_auc_rf:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "D4DbhiH6dB2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Display all model results ---\n",
        "print(\"\\n--- Model Comparison ---\")\n",
        "\n",
        "# Ensure model_results is populated. If you ran cells out of order or restarted,\n",
        "# you might need to re-run the previous model cells to populate it correctly.\n",
        "# For demonstration, I'll reconstruct it with the values you've provided\n",
        "# In your notebook, if model_results was built correctly, this part is just pd.DataFrame(model_results).T\n",
        "\n",
        "# Reconstructing model_results based on the outputs you shared:\n",
        "model_results_data = {\n",
        "    'Logistic Regression': {'Accuracy': 0.8041, 'Precision': 0.6541, 'Recall': 0.5561, 'F1-Score': 0.6012, 'ROC AUC': 0.8425},\n",
        "    'Decision Tree': {'Accuracy': 0.7253, 'Precision': 0.4825, 'Recall': 0.4786, 'F1-Score': 0.4805, 'ROC AUC': 0.6460},\n",
        "    'Random Forest': {'Accuracy': 0.7850, 'Precision': 0.6187, 'Recall': 0.4947, 'F1-Score': 0.5498, 'ROC AUC': 0.8248}\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(model_results_data).T # Transpose for better readability\n",
        "print(results_df.sort_values(by='ROC AUC', ascending=False))"
      ],
      "metadata": {
        "id": "bhQ6ELEfdB40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure your Random Forest model (rf_clf) and X_train (to get column names) are available\n",
        "# from the previous steps.\n",
        "\n",
        "if 'rf_clf' in globals() and 'X_train' in globals():\n",
        "    # Get feature importances from the trained Random Forest model\n",
        "    importances = rf_clf.feature_importances_\n",
        "\n",
        "    # Create a pandas Series to associate importances with feature names\n",
        "    # X_train.columns contains the names of all features after preprocessing (including one-hot encoded ones)\n",
        "    feature_names = X_train.columns\n",
        "    feature_importance_series = pd.Series(importances, index=feature_names)\n",
        "\n",
        "    # Sort the features by importance in descending order\n",
        "    sorted_feature_importances = feature_importance_series.sort_values(ascending=False)\n",
        "\n",
        "    print(\"Feature Importances from Random Forest:\\n\")\n",
        "    print(sorted_feature_importances)\n",
        "\n",
        "    # Visualize the top N most important features\n",
        "    N = 15 # You can choose how many top features to display\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.barplot(x=sorted_feature_importances.head(N).values, y=sorted_feature_importances.head(N).index)\n",
        "    plt.xlabel(\"Importance Score\")\n",
        "    plt.ylabel(\"Features\")\n",
        "    plt.title(f\"Top {N} Most Important Features (Random Forest)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Please make sure you have trained the Random Forest model ('rf_clf') and have 'X_train' available.\")"
      ],
      "metadata": {
        "id": "PHZ9qtrJdB7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# --- Hyperparameter Tuning for Random Forest ---\n",
        "print(\"--- Starting Hyperparameter Tuning for Random Forest ---\")\n",
        "print(\"This might take a few minutes depending on the grid size and your system...\\n\")\n",
        "\n",
        "# Define the parameter grid to search\n",
        "# These are just example values; a more exhaustive search would include more options\n",
        "# We'll keep the grid small for now to reduce runtime\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200],          # Number of trees\n",
        "    'max_depth': [None, 10, 20],         # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5],         # Minimum samples required to split a node\n",
        "    'min_samples_leaf': [1, 2],          # Minimum samples required at each leaf node\n",
        "    'max_features': ['sqrt', 'log2']     # Number of features to consider at every split\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest Classifier (the one we used before, but it will be re-instantiated by GridSearchCV)\n",
        "rf_clf_for_tuning = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# Scoring: 'roc_auc' is a good general metric. For churn, 'recall_weighted' or 'f1_weighted' can also be good.\n",
        "# We can also create a specific recall scorer for the positive class if needed. Let's stick to roc_auc for broadness.\n",
        "# cv=3 means 3-fold cross-validation. Can increase for more robust results, but increases time.\n",
        "grid_search_rf = GridSearchCV(estimator=rf_clf_for_tuning,\n",
        "                              param_grid=param_grid_rf,\n",
        "                              scoring='roc_auc', # Focus on ROC AUC for overall discrimination\n",
        "                              cv=3,              # Number of cross-validation folds\n",
        "                              verbose=1,         # Prints updates as it runs\n",
        "                              n_jobs=-1)         # Use all available CPU cores\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "# This is the step that takes time!\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters found\n",
        "print(\"\\nBest Hyperparameters found by GridSearchCV:\")\n",
        "print(grid_search_rf.best_params_)\n",
        "\n",
        "# Get the best estimator (the model with the best parameters)\n",
        "best_rf_model = grid_search_rf.best_estimator_\n",
        "\n",
        "print(\"\\n--- Evaluating Tuned Random Forest Model ---\")\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_best_rf = best_rf_model.predict(X_test)\n",
        "y_pred_proba_best_rf = best_rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the tuned model\n",
        "accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)\n",
        "precision_best_rf = precision_score(y_test, y_pred_best_rf)\n",
        "recall_best_rf = recall_score(y_test, y_pred_best_rf)\n",
        "f1_best_rf = f1_score(y_test, y_pred_best_rf)\n",
        "roc_auc_best_rf = roc_auc_score(y_test, y_pred_proba_best_rf)\n",
        "\n",
        "# Update our model_results dictionary (or create a new one for tuned models)\n",
        "if 'model_results' not in globals(): # Just in case it wasn't run in this session\n",
        "    model_results = {}\n",
        "model_results['Random Forest (Tuned)'] = {\n",
        "    'Accuracy': accuracy_best_rf,\n",
        "    'Precision': precision_best_rf,\n",
        "    'Recall': recall_best_rf,\n",
        "    'F1-Score': f1_best_rf,\n",
        "    'ROC AUC': roc_auc_best_rf\n",
        "}\n",
        "\n",
        "print(f\"Accuracy: {accuracy_best_rf:.4f}\")\n",
        "print(f\"Precision: {precision_best_rf:.4f}\")\n",
        "print(f\"Recall: {recall_best_rf:.4f}\")\n",
        "print(f\"F1-Score: {f1_best_rf:.4f}\")\n",
        "print(f\"ROC AUC Score: {roc_auc_best_rf:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_best_rf))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_best_rf))\n",
        "\n",
        "# --- Compare with the original Random Forest ---\n",
        "print(\"\\n--- Comparison with Original Random Forest ---\")\n",
        "original_rf_metrics = model_results.get('Random Forest', {}) # Get original RF if available\n",
        "tuned_rf_metrics = model_results['Random Forest (Tuned)']\n",
        "\n",
        "comparison_data = {\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC'],\n",
        "    'Original RF': [original_rf_metrics.get(m, 'N/A') for m in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC']],\n",
        "    'Tuned RF': [tuned_rf_metrics.get(m, 'N/A') for m in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC']]\n",
        "}\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(comparison_df)"
      ],
      "metadata": {
        "id": "kQI8NqPZdB-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}